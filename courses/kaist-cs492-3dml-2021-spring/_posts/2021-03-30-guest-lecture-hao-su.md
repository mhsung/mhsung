---
title:  "Compositional Generalizability in Geometry, Physics, and Policy Learning"

presenter: <a href="https://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>

schedule: Mar 30, 2021 (Tue), 9:00 a.m. KST

test: {{ page.path }}

slug: guest-lecture-hao-su

date: 2021-01-15
---


## {{ page.title }}
{:.title}
### {{ page.presenter }}
{:.title}
#### {{ page.schedule }}
{:.title}
<br />

#### Guest Lecture at <a href="{{site.baseurl}}/{{page.path}}/../../main/" target="_blank">CS492(H) Machine Learning for 3D Data (Spring 2021)</a>, <a href="https://www.kaist.ac.kr/" target="_blank">KAIST</a>.
{:.title}
#### Host: <a href="{{site.baseurl}}/" target="_blank">Minhyuk Sung</a>
{:.title}
<br />

![]({{site.baseurl}}/{{page.path}}/../../images/guest-lecture-hao-su.png)
<br />

### Abstract
It is well known that deep neural networks are universal function approximators and have good generalizability when the training and test datasets are sampled from the same distribution. Most deep learning-based applications and theories in the past decade are based upon this setup. While the view of learning function approximators has been rewarding to the community, we are seeing more and more of its limitations when dealing with the real-world problem space that is combinatorially exploded. In this talk, I will discuss a possible shift of view, from learning function approximators to learning algorithm approximators, by some preliminary work in my lab. Our ultimate goal is to achieve generalizability when learning in a problem space of combinatorial complexity. We refer to this desired generalizability as compositional generalizability. To this goal, we take important problems in geometry, physics, and policy learning as testbeds. Particularly, I will introduce how we build algorithms with state-of-the-art compositional generalizability on these testbeds, following a bottom-up principle and a modularized principle.
<br />


#### Zoom Webinar Link: TBA
<br />

